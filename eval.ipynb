{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag_evaluation_experiment-65e032b8' at:\n",
      "https://smith.langchain.com/o/f34f0648-f907-4864-b09a-dd5f87740bd9/datasets/96a47dfc-d5cc-422a-8ea3-42cf694a2418/compare?selectedSessions=06913239-6638-44b7-91c3-dc1f1f386bca\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69947a85f1c244d58af8fd0374e60a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable, evaluate\n",
    "from langsmith.schemas import Example, Run\n",
    "from dotenv import load_dotenv\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "wcd_url = os.environ[\"WCD_URL\"]\n",
    "wcd_api_key = os.environ[\"WCD_API_KEY\"]\n",
    "client = wrap_openai(OpenAI())\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=wcd_url,                                    \n",
    "    auth_credentials=Auth.api_key(wcd_api_key),\n",
    ")\n",
    "openai_client = wrap_openai(OpenAI(api_key=os.getenv('OPENAI_API_KEY')))\n",
    "\n",
    "COLLECTION_NAME = \"James\"\n",
    "GPT_MODEL_NAME = \"gpt-4o\"\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-3-large\"\n",
    "DATASET_NAME = \"rag_evaluation_dataset\"\n",
    "EXPERIMENT_PREFIX = \"rag_evaluation_experiment\"\n",
    "\n",
    "@traceable\n",
    "def answer_with_rag(inputs: dict) -> dict:\n",
    "    query = inputs[\"messages\"][-1][\"content\"]\n",
    "\n",
    "    # Get embedding for the query\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL_NAME,\n",
    "        input=query\n",
    "    )\n",
    "    query_embedding = response.data[0].embedding\n",
    "\n",
    "    # Search for similar texts in Weaviate\n",
    "    collection = client.collections.get(COLLECTION_NAME)\n",
    "    similar_texts = collection.query.near_vector(\n",
    "        near_vector=query_embedding,\n",
    "        limit=3,\n",
    "        return_properties=[\"text\"],\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "\n",
    "    # Prepare context for GPT\n",
    "    context_str = \"\\n\\n---\\n\\n\".join([doc.properties[\"text\"] for doc in similar_texts.objects])\n",
    "\n",
    "    prompt = f\"\"\"Answer the question using ONLY the information provided in the context below. \n",
    "    Do not add any general knowledge or information not contained in the context.\"\n",
    "\n",
    "    Context:\n",
    "    {context_str}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    # Generate answer using GPT-4\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=GPT_MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"message\": {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "    }\n",
    "\n",
    "\n",
    "def correctness_evaluator(run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates the correctness of generated response.\n",
    "\n",
    "    Args:\n",
    "        run: Contains the run information including inputs and outputs\n",
    "        example: Contains the reference example if available\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with score (0-1) and explanation\n",
    "    \"\"\"\n",
    "    # Extract the original vocabulary list from inputs\n",
    "    query = run.inputs[\"inputs\"][\"messages\"][-1][\"content\"]\n",
    "\n",
    "    # Extract the model's generated dialogue\n",
    "    answer = run.outputs[\"message\"][\"content\"]\n",
    "\n",
    "    # Rest of the evaluation logic remains the same\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    Given a query by the user and two responses, evaluate whether the two responses are basically equivalent and whether the second response satisfactorily answers the query.\n",
    "\n",
    "    Use the following scoring rubric:\n",
    "    2 = The two responses are equivalent and the second response satisfactorily answers the query.\n",
    "    1 = The two responses are not equivalent but the second response does answer the query.\n",
    "    0 = The two responses are not equivalent and the second response does not answer the query.\n",
    "    \n",
    "    Return only the number (0-2).\n",
    "\n",
    "    The query is: {query}\n",
    "\n",
    "    The first response is: {example.outputs[\"message\"][\"content\"]}\n",
    "\n",
    "    The second response is: {answer}\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a dialogue evaluation assistant. Respond only with a number 0-2.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        score = int(response.choices[0].message.content.strip())\n",
    "        return {\n",
    "            \"key\": \"correctness score\",\n",
    "            \"score\": score / 2,  # Normalize to 0-1\n",
    "            \"explanation\": f\"Correctness score: {score}/2\",\n",
    "        }\n",
    "    except ValueError:\n",
    "        return {\n",
    "            \"key\": \"correctness score\",\n",
    "            \"score\": 0,\n",
    "            \"explanation\": \"Failed to parse score\",\n",
    "        }\n",
    "\n",
    "\n",
    "# List of evaluators to score the outputs of target task\n",
    "evaluators = [correctness_evaluator]\n",
    "\n",
    "# Evaluate the target task\n",
    "results = evaluate(\n",
    "    answer_with_rag,\n",
    "    data=DATASET_NAME,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=EXPERIMENT_PREFIX,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
